{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0883a537",
   "metadata": {},
   "source": [
    "# SNLP Assignment 6\n",
    "\n",
    "Name 1: <br>\n",
    "Student id 1: <br>\n",
    "Email 1: <br>\n",
    "\n",
    "Name 2: <br>\n",
    "Student id 2:  <br>\n",
    "Email 2:  <br>\n",
    "\n",
    "Name 3: <br>\n",
    "Student id 3:  <br>\n",
    "Email 3: <br>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br>\n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ce2aa",
   "metadata": {},
   "source": [
    "### General guidelines:\n",
    "1. Don't change the layout of the notebook, especially the final output cells. If you do end up changing the code, add comments to tell us why you needed to do it.\n",
    "2. Use log with base 2 wherever you feel logarithimic operations are necessary. If you do use other bases, please specify it and explain why.\n",
    "3. For tokenizers, it is sufficient to use tokenizers from `nltk` library.\n",
    "4. For the theoritical questions where no math is involved, it is sufficient to answer the questions with 3-4 sentences. Do not be too verbose.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2c383",
   "metadata": {},
   "source": [
    "There are many different ways to smooth language models for managing OOV words. In this assignment, we will be looking at some of these smoothing methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm.notebook import tqdm ##install tqdm if not available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea59152",
   "metadata": {},
   "source": [
    "## Exercise 6.3:  Implement an Interpolated N-gram Model: \n",
    "\n",
    "1. In the provided `exercise_3.py` file, you are required to implement an interpolated n-gram model that uses        \n",
    "   add-$\\alpha$ smoothing. For this exercise, we are using **Penn-Treebank corpus** present in the `nltk` library. Here are the specific steps you need to implement:                                               (**Total**: 1.5 points)\n",
    "\n",
    "    - `load_and_preprocess_data`: provided to you. \n",
    "    - `make_vocab`: fill in the function to create a vocabulary for our language models. (0.25 points)\n",
    "    - `restrict_vocab`: fill in the function to restrict the corpus to fit inside the vocabulary using <unk> for OOV words. (0.25 points)\n",
    "    - `train_test_split`: split the corpus into train and test sets. This is for the next exercise. (0.25 points)\n",
    "\n",
    "    - `Interpolated_Model` class: We have defined a class `Interpolated_Model` for you. It is intilialized with values $\\alpha = 0$, n=2 i.e. bigrams, interpolation weight = $\\frac{1}{n}$. This class should have methods for initializing the model, calculating Laplace smoothed probabilities, calculating interpolated log probabilities, getting n-grams, and calculating perplexity on the test set. The codes for preparing the corpus are already there. Your job is to complete the following functions :\n",
    "        - `_get_n_grams`: provided to you.\n",
    "        - `laplace_prob`: return the log proabability of an ngram. Adjust this function for (add-$\\alpha$) Smoothing. (0.25 poimts)\n",
    "        - `interpolated_logprob`: return the log proabability of the interpolated ngram model. This should be using the add-$\\alpha$ smoothed probabilites. (0.25 poimts)\n",
    "        - `perplexity`: calculates the perplexity of the model on the test sentences. (0.25 points)\n",
    "\n",
    "    Once complete, run it on the supplied parameters ($\\alpha = 0.0001$ and $n=3$) and report the perplexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094759be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary module\n",
    "import exercise_3_solutions\n",
    "\n",
    "# Reload the module to reflect any changes made to the file\n",
    "exercise_3 = reload(exercise_3_solutions)\n",
    "\n",
    "# Load and preprocess the data. The maximum n-gram order is set to 10.\n",
    "corpus = exercise_3.load_and_preprocess_data(max_ngram_order=10)\n",
    "\n",
    "# Split the corpus into training and testing sets. The split is 70:30 and the test set is taken from the last 30% of the corpus.\n",
    "# Note: The data is not randomized at this stage.\n",
    "train, test = exercise_3.train_test_split(corpus, split=0.7)\n",
    "\n",
    "# Create the vocabulary from the training set. The vocabulary consists of the top 5000 most frequent words.\n",
    "vocab = exercise_3.make_vocab(train, 5000)\n",
    "\n",
    "# Restrict the vocabulary of the training and testing sets. Words not in the vocabulary are replaced with '<unk>'.\n",
    "vocabulary_restricted_train = exercise_3.restrict_vocab(train, vocab)\n",
    "vocabulary_restricted_test = exercise_3.restrict_vocab(test, vocab)\n",
    "\n",
    "# Initialize the Interpolated_Model class with the vocabulary-restricted training and testing sets.\n",
    "# The smoothing factor alpha is set to 0.0001 and the n-gram order is set to 3.\n",
    "# Note: If you want to increase the n-gram order higher than 10, you will need to edit the load_and_preprocess_data() function.\n",
    "\n",
    "n_order = 3\n",
    "alpha_val = 0.0001\n",
    "interpolated_model = exercise_3.Interpolated_Model(train_sents=vocabulary_restricted_train, test_sents=vocabulary_restricted_test, alpha= alpha_val, order= n_order)\n",
    "\n",
    "# Calculate and print the perplexity of the model.\n",
    "print(f\"Perplexity of interpolated model with alpha {alpha_val} and n-gram order {n_order}: {interpolated_model.perplexity()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27360bb1",
   "metadata": {},
   "source": [
    "2. **Hyperparameter Tuning**: (1 point)\n",
    "\n",
    "    After implementing the interpolated n-gram model, you need to find the optimal n-gram order using 5-fold cross-validation. You need to find the optimal $n$ from the provided selection here: {1,2,3,4,5}\n",
    "\n",
    "    Perform 5-fold cross validation and find the optimal values. Plot the perplexity against the hyperparameter values ($\\alpha$ and n-gram order) to show the reasoning behind your choice. Set $\\alpha$ = 0.0001 for this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: This code block may take some time to run. For me, it took 10 minutes\n",
    "\n",
    "num_folds = 5\n",
    "fold_size = len(corpus) // num_folds\n",
    "ppl_per_order = []\n",
    "\n",
    "for ngram_ in tqdm(range(1, 5), desc=\"N-gram order\"):\n",
    "    fold_ppls = []\n",
    "    for fold in tqdm(range(num_folds), desc=f\"Folds for n={ngram_}\", leave=False):\n",
    "        # Split indices for test and train\n",
    "        test_start = fold * fold_size\n",
    "        test_end = (fold + 1) * fold_size if fold < num_folds - 1 else len(corpus)\n",
    "        test_indices = list(range(test_start, test_end))\n",
    "        train_indices = [i for i in range(len(corpus)) if i not in test_indices]\n",
    "        \n",
    "        train = [corpus[i] for i in train_indices]\n",
    "        test = [corpus[i] for i in test_indices]\n",
    "        \n",
    "        vocab = exercise_3.make_vocab(train, 5000)\n",
    "        vocabulary_restricted_train = exercise_3.restrict_vocab(train, vocab)\n",
    "        vocabulary_restricted_test = exercise_3.restrict_vocab(test, vocab)\n",
    "        \n",
    "        interpolated_model = exercise_3.Interpolated_Model(\n",
    "            train_sents=vocabulary_restricted_train,\n",
    "            test_sents=vocabulary_restricted_test,\n",
    "            alpha=0.0001,\n",
    "            order=ngram_\n",
    "        )\n",
    "        fold_ppls.append(interpolated_model.perplexity())\n",
    "    ppl_per_order.append(np.mean(fold_ppls))\n",
    "\n",
    "plt.plot(range(1, 10), ppl_per_order)\n",
    "plt.xlabel('N-gram Order')\n",
    "plt.ylabel('Perplexity')\n",
    "plt.title('Perplexity vs N-gram Order (5-fold CV)')\n",
    "plt.show()\n",
    "\n",
    "optimal_index = np.argmin(ppl_per_order)\n",
    "optimal_order = list(range(1, 10))[optimal_index]\n",
    "optimal_perplexity = ppl_per_order[optimal_index]\n",
    "\n",
    "print(f\"The optimal n-gram order is {optimal_order} with a perplexity of {optimal_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d91e97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e775745c",
   "metadata": {},
   "source": [
    "## Exercise 6.4: Kneser-Ney vs the World\n",
    "\n",
    "In this exercise, you'll implement different smoothing techniques for a trigram language model and compare their behavior using text from **alice_in_wonderland.txt**. The goal is to understand why advanced smoothing methods like Kneser-Ney often outperform simpler approaches. Here's what you need to implement in `exercise_4.py`: (**Total**: 4.5 points)\n",
    "\n",
    "1. `preprocess_text`: Preprocess the text by first removing all punctuation and newlines, lowercasing, and then tokenisation. There is no need to split the data into train and test sets. Replace rare words with counts lower than a given threshold. (0.5 points)\n",
    "\n",
    "2. `SmoothingCounter` class: A class for n-gram language modeling that provides both Kneser-Ney and add-alpha (Laplace) smoothing for bigram and trigram probabilities. The initialization should include the discounting param `d`. Set it to $0.75$. It is recoemmended to set up Counters for tracking unigram/bigram/trigram counts here. (0.5 points)\n",
    "\n",
    "3. `prob_good_turing_bigram` & `prob_good_turing_trigram`: Compute Good-Turing smoothed probabilities for bigrams and trigrams. (1 point)\n",
    "\n",
    "4. `knprob_bigram` & `knprob_trigram`: Functions that calculate the bigram ($P_{KN}(w_3|w_2)$) and trigram probability ($P_{KN}(w_3|w_2,w_1)$) using the equations from Exercise 6.2. (0.5 point)\n",
    "\n",
    "5. `prob_alpha_bigram` & `prob_alpha_trigram`: Functions that calculate the log prob of a bigram and trigram with counts adjusted for add-$\\alpha$ smoothing. (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ab333e",
   "metadata": {},
   "source": [
    "Now, using these, consider the trigrams: (1 point)\n",
    "   - \"alice said nothing\"\n",
    "   - \"alice said nichts\".\n",
    "\n",
    "   For these trigrams, use your newly defined functions to calculate the smoothed bigram and trigram probabilities $P_{KN}(nothing|said)$ ,$P_{KN}(nichts|said)$, $P_{KN}(nothing|said,alice)$ and $P_{KN}(nichts|said,alice)$.<br>\n",
    "   Also complete the functions to find the same bigram and trigram probabilites using add-$\\alpha$ smoothing. Set the $\\alpha = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921eaa6e",
   "metadata": {},
   "source": [
    "**NOTE**: \n",
    "1. For Good-Turing, refresh your understanding of how it works from this [video](https://www.youtube.com/watch?v=GwP8gKa-ij8&t=2s).\n",
    "2. You are free to add extra helper functions to the `SmoothingCounter` class, but provide detailed comments on what those functions are doing. \n",
    "2. Don't forget to answer the given question at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4153e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_4\n",
    "exercise_4 = reload(exercise_4)\n",
    "\n",
    "file = open(\"data/alice_in_wonderland.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# TODO: Preprocess text\n",
    "tokens = exercise_4.preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram1 = ('alice', 'said', 'nothing')\n",
    "trigram2 = ('alice', 'said', 'nichts')\n",
    "\n",
    "bigram1 = trigram1[1:]\n",
    "bigram2 = trigram2[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kn = exercise_4.SmoothingCounter(tokens, alpha=0)      \n",
    "model_alpha = exercise_4.SmoothingCounter(tokens, alpha=0.5)\n",
    "\n",
    "print(f\"{trigram1} Probs with KN-smoothing: {model_kn.knprob_trigram(trigram1)}\")\n",
    "print(f\"{trigram1} Probs with alpha-smoothing: {model_alpha.prob_alpha_trigram(trigram1)}\")\n",
    "print(f\"{trigram1} Probs with Good-Turing smoothing: {model_kn.prob_good_turing_trigram(trigram1)}\")\n",
    "print()\n",
    "print(f\"{trigram2} Probs with KN-smoothing: {model_kn.knprob_trigram(trigram2)}\")\n",
    "print(f\"{trigram2} Probs with alpha-smoothing: {model_alpha.prob_alpha_trigram(trigram2)}\")\n",
    "print(f\"{trigram2} Probs with Good-Turing smoothing: {model_kn.prob_good_turing_trigram(trigram2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58927d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kn = exercise_4.SmoothingCounter(tokens, alpha=0)  \n",
    "model_alpha =  exercise_4.SmoothingCounter(tokens, alpha=0.5) \n",
    "\n",
    "print(f\"{bigram1} Probs with KN-smoothing: {model_kn.knprob_bigram(bigram1)}\")\n",
    "print(f\"{bigram1} Probs with alpha-smoothing: {model_alpha.prob_alpha_bigram(bigram1)}\")\n",
    "print(f\"{bigram1} Probs with Good-Turing smoothing: {model_kn.prob_good_turing_bigram(bigram1)}\")\n",
    "print()\n",
    "print(f\"{bigram2} Probs with KN-smoothing: {model_kn.knprob_bigram(bigram2)}\")\n",
    "print(f\"{bigram2} Probs with alpha-smoothing: {model_alpha.prob_alpha_bigram(bigram2)}\")\n",
    "print(f\"{bigram2} Probs with Good-Turing smoothing: {model_kn.prob_good_turing_bigram(bigram2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212bb9a",
   "metadata": {},
   "source": [
    "**Q**. What can you conclude from these results? Can you provide an explanation why this is happening? (0.5 points)\n",
    "\n",
    "**A**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
