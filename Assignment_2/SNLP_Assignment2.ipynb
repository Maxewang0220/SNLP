{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace9afaa",
   "metadata": {},
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "**NOTE**: For this specific exercise, you are not allowed to import any libraries other than the ones already specified in the next cell\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e327e",
   "metadata": {},
   "source": [
    "## Exercise 1 - Theoretical Concepts (4 points)\n",
    "\n",
    "### 1.1 Probabilistic Independence (2 points)\n",
    "\n",
    "Consider a fair 6-sided die whose sides are numbered from 1 to 6 and each die roll is independent of the other rolls. In an experiment that consists of rolling the die twice, the following events can be defined:\n",
    "\n",
    "    \n",
    "    A: The sum of the two outcomes is strictly larger than 10\n",
    "    B: At least one of the two rolls resulted in 6\n",
    "    C: At least one of the two rolls resulted in 3\n",
    "    D: The outcome of the 2nd roll was higher than the 1st roll\n",
    "    E: The difference between the two roll outcomes is exactly 1\n",
    "\n",
    "a. Compute the probabilities $P(A)$, $P(C)$, and $P(E)$.\n",
    "\n",
    "b. Is event A independent of event B?\n",
    "\n",
    "c. Is event A independent of event C?\n",
    "\n",
    "d. Are events D and E independent?\n",
    "\n",
    "Justify your answers using the laws of probability and the definition of probabilistic independence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caaa63a",
   "metadata": {},
   "source": [
    "### 1.2 Bayes Theorem (2 points)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory used to update the probability of a hypothesis based on new evidence. It states that the probability of a hypothesis (H) given some evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, divided by the probability of the evidence regardless of the hypothesis. Mathematically,\n",
    "\n",
    "$$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $$\n",
    "\n",
    "Where,\n",
    "- $P(H∣E)$ is the posterior probability of hypothesis H given evidence E.\n",
    "- $P(E∣H)$ is the probability of observing evidence E given that the hypothesis H is true.\n",
    "- $P(H)$ is the prior probability of hypothesis H being true.\n",
    "- $P(E)$ is the probability of observing evidence E.\n",
    "\n",
    "\n",
    "Now, consider a hospital where patients are tested for a rare disease. The disease is known to affect $2\\%$ of the population. A diagnostic test is available with the following accuracy:\n",
    "\n",
    "If a person has the disease, the test returns a positive result $95\\%$ of the time (true positive rate).\n",
    "\n",
    "If a person does not have the disease, the test still gives a false positive $5\\%$ of the time.\n",
    "\n",
    "a. What is the prior probability that a randomly selected individual has the disease?\n",
    "\n",
    "b. If a randomly selected individual tests positive, what is the probability that they actually have the disease?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa1222",
   "metadata": {},
   "source": [
    "## Exercise 2 Perplexity (3 points)\n",
    "\n",
    "The perplexity of a model can also be defined as $2^{-\\frac{1}{n} \\sum^n_1 \\log p(w_i|w_{i-1})}$. For the following exercise, use the log probabilities given of a pretrained bigram language model. Tokenization is apparent from the tokens in the following table.\n",
    "\n",
    "|A|B|log p(B\\|A)|\n",
    "|-|-|-|\n",
    "|`The`|`man`|-1.8|\n",
    "|`the`|`man`|-2.2|\n",
    "|`the`|`post`|-2.7|\n",
    "|`Man`|`the`|-5.1|\n",
    "|`man`|`the`|-3.7|\n",
    "|`man`|`shouted`|-2.9|\n",
    "|`shouted`|`\"`|-3.1|\n",
    "|`post`|`!`|-3.1|\n",
    "|`\"`|`Man`|-1.9|\n",
    "|`\"`|`man`|-1.7|\n",
    "|`!`|`\"`|-1.2|\n",
    "|`\"`|`The`|-0.9|\n",
    "|`\"`|`the`|-1.2|\n",
    "\n",
    "Assume probabilities not listed are $0^+$ (and the respective logarithm $-\\infty$). For counting bigrams, consider your corpus as a circular structure i.e. include the bigram $(w_N, w_1)$ in your final counts. Therefore the weight of each bigram is $\\frac{1}{|\\text{words}|}$.\n",
    "\n",
    "### 2.1 Lowercasing Input (1.5 points)\n",
    "\n",
    "Compute the perplexity of the following two sentences (and show the steps).\n",
    "\n",
    "```\n",
    "The man shouted \"Man the post!\"\n",
    "the man shouted \"man the post!\"\n",
    "```\n",
    "\n",
    "Is lowercasing the input always a good idea? What are the advantages and disadvantages?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4bdb6",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Unknown Tokens (1.5 points)\n",
    "\n",
    "Compute the perplexity of the following two sentences.\n",
    "\n",
    "```\n",
    "The man shouted \"Man the stations!\"\n",
    "The man shouted \"Man the the!\"\n",
    "```\n",
    "\n",
    "Elaborate on the computed results. 2. Do you consider both sentences to be equally probable?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdef1cd6",
   "metadata": {},
   "source": [
    "## Exercise 3 - Comparing distributions (6 points)\n",
    "\n",
    "In this exercise you will compare the probability distributions $P(w_i|w_{i−1} = ``to\")$ and $P(w_i|w_{i−1} = ``the\")$ . The distribution of the words given the previous word is \"to\" or \"the\" respectively.\n",
    "\n",
    "1. Download the Brown corpus from the web (http://www.nltk.org/nltk_data/) or through the python NLTK toolkit. (0.5 points)\n",
    "1. Tokenize and lowercase each token. (0.5 points)\n",
    "1. Estimate the conditional probability distributions $P(w_i|w_{i−1} = ``to\")$ and $P(w_i|w_{i−1} = ``the\")$\n",
    "with maximum likelihood estimation. (2 point)\n",
    "1. Plot the frequency distribution (unnormalized frequency counts) or the probability distribution for the 50 most frequent tokens for both distributions. Based on the plots which distribution do you expect to have a higher entropy? Justify your answer. (1 points)\n",
    "1. Calculate the entropy for both distributions. Was your guess accurate? Elaborate on why it was right/wrong. (1 point)\n",
    "\n",
    "\n",
    "Some remarks:\n",
    "\n",
    "* Though not crucial, remember to use a base 2 logarithm, i.e. $\\log_2$.\n",
    "* Your code should be fast enough to tokenize and get the bigram statistics for the Brown corpus in a few seconds.\n",
    "* A bar plot would be ideal to visualize the probabilities/counts for the top 50 words.\n",
    "* If you need to modify the boilerplate code below, comment why. **You are allowed to change the provided code, both in the notebook and in the python file, but please explain why you did.**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec419c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_3_and_4\n",
    "exercise_3_and_4 = reload(exercise_3_and_4)\n",
    "\n",
    "tokenized_corpus = exercise_3_and_4.load_corpus()\n",
    "top_50 = exercise_3_and_4.get_top_n_probabilities(tokenized_corpus, ['the', 'to'], 50)\n",
    "\n",
    "# Feel free to make the printing of the result \"prettier\"\n",
    "print(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequencies or distributions of the top 50 following words\n",
    "exercise_3_and_4.plot_top_n(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89269ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the entropy\n",
    "print(exercise_3_and_4.get_entropy(top_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebedba",
   "metadata": {},
   "source": [
    "## Exercise 4 - Mean rank (2 points)\n",
    "\n",
    "With the distributions from exercise 3, calculate the perplexity and mean rank of the following phrases:\n",
    "\n",
    "* \"the election\"\n",
    "* \"the jury\"\n",
    "* \"the administration\"\n",
    "* \"the to\"\n",
    "* \"the the\"\n",
    "\n",
    "How close are the two metrics? Which one would you use?\n",
    "\n",
    "Note: Assume that bi-grams not present have a $P$ of $0^+$, and therefore a logarithm value of $-\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041eca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will probably need to pass more arguments aside from just the bigram\n",
    "# to be evaluated to each of these calls.\n",
    "\n",
    "# Get perplexity scores\n",
    "exercise_3_and_4.get_perplexity(\"the election\")\n",
    "exercise_3_and_4.get_perplexity(\"the jury\")\n",
    "exercise_3_and_4.get_perplexity(\"the administration\")\n",
    "exercise_3_and_4.get_perplexity(\"the to\")\n",
    "exercise_3_and_4.get_perplexity(\"the the\")\n",
    "\n",
    "# Get mean rank scores\n",
    "exercise_3_and_4.get_mean_rank(\"the election\")\n",
    "exercise_3_and_4.get_mean_rank(\"the jury\")\n",
    "exercise_3_and_4.get_mean_rank(\"the administration\")\n",
    "exercise_3_and_4.get_mean_rank(\"the to\")\n",
    "exercise_3_and_4.get_mean_rank(\"the the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e5402",
   "metadata": {},
   "source": [
    "## Exercise 5 - Miller's model (3 points)\n",
    "\n",
    "Miller's model introduced in the lecture simulates the process of random text generation at the character level. It is a memoryless model which means that the generation at each time step is independent of what has been generated so far. Imagine that there is a monkey typing on a computer keyboard. The keyboard has only 27 keys: a to z and a spacebar. Assume that the probability of each character is distributed according to its occurrence in the English language. We call a sequence of characters separated by white space as a word.\n",
    "\n",
    "a. What is the probability that the monkey will type the word _'bayes'_ on the keyboard? Hint: See the `exercise_5.py` file.\n",
    "\n",
    "b. Complete the `compute_perplexity()` function in the `exercise_5.py` file to compute the perplexity of the Miller's model for the text: \"i am an intelligent monkey\". \n",
    "\n",
    "c. What would happen to the perplexity if we were to add a few more whitespace in between the words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25157102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.b\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_5.py\n",
    "import exercise_5\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_5)\n",
    "\n",
    "model = exercise_5.MillersModel()\n",
    "\n",
    "text = \"i am an intelligent monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
