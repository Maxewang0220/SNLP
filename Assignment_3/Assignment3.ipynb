{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495997a0",
   "metadata": {},
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: Entang Wang\n",
    "\n",
    "Student id 1: 7069521\n",
    "\n",
    "Email 1:\n",
    "\n",
    "Name 2: \n",
    "\n",
    "Student id 2: \n",
    "\n",
    "Email 2:  \n",
    "\n",
    "Instructions: Read each question carefully. \n",
    "\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. \n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of Name1_studentID1_Name2_studentID2.zip. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after your upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas==1.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "from math import log2\n",
    "from typing import List, Dict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2676c",
   "metadata": {},
   "source": [
    "## Exercise 1: Cross-Entropy and KL-Divergence (6 points)\n",
    "\n",
    "### Theory\n",
    "\n",
    "Recall the formulas for Cross-Entropy:\n",
    "\n",
    "$$H(P, Q) = -\\sum_{x \\in X} P(x) \\times \\log{Q(x)}$$\n",
    "\n",
    "And KL-Divergence:\n",
    "\n",
    "$$D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\times \\log{\\frac{P(x)}{Q(x)}}$$\n",
    "\n",
    "where P often signifies the empirical or observed probability distribution and Q signifies the estimated distribution.\n",
    "\n",
    "\n",
    "1. A common way to train a language model is to minimize the Cross-Entropy score. Explain why minimizing Cross-Entropy is equivalent to minimizing KL-Divergence. Support your answer with a mathematical expression. [1 point]\n",
    "\n",
    "2. For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
    "\n",
    "    $\\forall x,y,z \\in U:$\n",
    "\n",
    "    1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
    "    2. $d(x,y) = d(y,x)$\n",
    "    3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
    "\n",
    "    Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
    "For each of the three points either prove that it holds for KDLâ€‹ or show a counterexample why not. [2 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f502fb9",
   "metadata": {},
   "source": [
    "### Code\n",
    "\n",
    "Suppose we have three coins. Here are the results of flipping each coin 20 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f33f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin1 = \"H T H T H T H T H T H T H T H T H T H T\"\n",
    "coin2 = \"H H H H T H H T H H H T T H H T H H H H\"\n",
    "coin3 = \"T T T T T T T T T T T T T H T T T T H T\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e2aeb",
   "metadata": {},
   "source": [
    "Let's turn the sequences into lists of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s : str) -> List[str]:\n",
    "    return s.split()\n",
    "\n",
    "coin1_tokens = tokenize(coin1)\n",
    "coin2_tokens = tokenize(coin2)\n",
    "coin3_tokens = tokenize(coin3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335b2d6",
   "metadata": {},
   "source": [
    "Write the methods for a unigram model that\n",
    "1. Estimate a probability distribution, given the tokenized text (use the imported `Counter`). Make sure that it is possible to update the model's distribution. [0.5 points]\n",
    "2. Calculate the cross entropy between the model's estimated distribution and some given probability distribution. [1 point]\n",
    "3. Calculate the KL-Divergence. [1 point]\n",
    "\n",
    "**NOTE**: So far, we haven't covered strategies for dealing with out-of-vocabulary tokens. For now, we will accept if you:\n",
    " * Include only the tokens that are present in both distributions when calculating Cross-Entropy and KL-Divergence, i.e. ignore the tokens that don't appear in both distributions.\n",
    " * Normalize the resulting distributions so that they sum up to one.\n",
    "\n",
    "Feel free to write separate methods for those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c24b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.dist = {}\n",
    "        self.freq = {}\n",
    "    \n",
    "    def fit(self, data: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Define a probability distribution for the model\n",
    "        and assign it to self.dist\n",
    "        \n",
    "        Args:\n",
    "            data - list of tokens\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "    def cross_entropy(self, distribution: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate the Cross-Entropy between the model's and a given distribution\n",
    "        \n",
    "        Args:\n",
    "            distribution - dictionary of token probabilities\n",
    "        Returns:\n",
    "            cross_entropy - the Cross-Entropy value\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def kl_divergence(self, distribution: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate the KL divergence between the model's and a given distribution\n",
    "\n",
    "        Args:\n",
    "            distribution - dictionary of token probabilities\n",
    "        Returns:\n",
    "            kl_divergence - the KL-Divergence value\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478f16e6",
   "metadata": {},
   "source": [
    "Now fit the models on the provided coins and print out the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04472ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_model1 = UnigramModel()\n",
    "coin_model2 = UnigramModel()\n",
    "coin_model3 = UnigramModel()\n",
    "\n",
    "coin_model1.fit(coin1_tokens)\n",
    "coin_model2.fit(coin2_tokens)\n",
    "coin_model3.fit(coin3_tokens)\n",
    "\n",
    "print(\"Model 1 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model1.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Model 2 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model2.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Model 3 estimated probabilities\")\n",
    "for token, prob in sorted(coin_model3.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9d1f6",
   "metadata": {},
   "source": [
    "Update Model 2 with some additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin2_ext = \"H H T H H H T T H H H H H H H H H T T H H H H H H T H H H T H H H\"\n",
    "coin2_ext_tokens = tokenize(coin2_ext)\n",
    "coin_model2.fit(coin2_ext_tokens)\n",
    "\n",
    "print(\"Model 2 updated estimated probabilities\")\n",
    "for token, prob in sorted(coin_model2.dist.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{token}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696a26c",
   "metadata": {},
   "source": [
    "Let's assume the empirical probability distribution for all coins is actually uniform. Calculate the Cross-Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61163c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_coin_dist = {\n",
    "    \"H\": 0.5,\n",
    "    \"T\": 0.5\n",
    "}\n",
    "\n",
    "print(\"Cross-Entropy with Uniform Distribution\")\n",
    "print(\"Model 1 Cross-Entropy:\", coin_model1.cross_entropy(uniform_coin_dist))\n",
    "print(\"Model 2 Cross-Entropy:\", coin_model2.cross_entropy(uniform_coin_dist))\n",
    "print(\"Model 3 Cross-Entropy:\", coin_model3.cross_entropy(uniform_coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a86569",
   "metadata": {},
   "source": [
    "Try it out with another distribution of your choosing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e93256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coin_dist = {\n",
    "    \"H\": 0.1,\n",
    "    \"T\": 0.9\n",
    "}\n",
    "\n",
    "print(\"Cross-Entropy with Distribution:\")\n",
    "print(\"Model 1 Cross-Entropy:\", coin_model1.cross_entropy(coin_dist))\n",
    "print(\"Model 2 Cross-Entropy:\", coin_model2.cross_entropy(coin_dist))\n",
    "print(\"Model 3 Cross-Entropy:\", coin_model3.cross_entropy(coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d7b952",
   "metadata": {},
   "source": [
    "Calculate KL-Divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aedd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KL-Divergence with Uniform Distribution:\")\n",
    "print(\"Model 1 KL Divergence:\", coin_model1.kl_divergence(uniform_coin_dist))\n",
    "print(\"Model 2 KL Divergence:\", coin_model2.kl_divergence(uniform_coin_dist))\n",
    "print(\"Model 3 KL Divergence:\", coin_model3.kl_divergence(uniform_coin_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48356c11",
   "metadata": {},
   "source": [
    "In the `data` folder you are provided with:\n",
    "\n",
    "* `unigram_freq.csv`: a file containing information on ~300k top words and their counts, derived from the Google Web Trillion Word Corpus (comma-separated).\n",
    "* `frankenstein.txt`: the novel \"Frankenstein\" by Mary Shelly.\n",
    "* `wikipedia.txt`: English Wikipedia corpus.\n",
    "* `code.txt`: A small corpus of code, taken from the [codeparrot/github-code](https://huggingface.co/datasets/codeparrot/github-code) dataset.\n",
    "\n",
    "To load and tokenize the texts, feel free to reuse the functions you wrote in your first assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5cacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(filepath: str = 'data.txt') -> str:\n",
    "    \"\"\"\n",
    "    Load text from a file.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the file to be loaded.\n",
    "    Returns:\n",
    "        The content of the file as a string.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "def preprocess(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing and removing punctuation.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens after preprocessing.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a54e498",
   "metadata": {},
   "source": [
    "Load the `unigram_freq.csv` data (use `pandas`) and create a dictionary representing a probability distribution (word: probability). Put the dictionary into a variable `empirical_dist`. [0.5 points]\n",
    "\n",
    "For the sake of this exercise, we will assume it as the true probability distribution of American English and that no other words appear in the distribution apart from those listed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "empricial_dist ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c02864",
   "metadata": {},
   "source": [
    "Calculate the following Cross-Entropy and KL-Divergence Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_frankenstein = load_text('data/frankenstein.txt')\n",
    "tokens_frankenstein = preprocess(text_frankenstein)\n",
    "\n",
    "model_frankenstein = UnigramModel()\n",
    "model_frankenstein.fit(tokens_frankenstein)\n",
    "\n",
    "print(\"Frankenstein Cross-Entropy with Empirical Distribution:\", model_frankenstein.cross_entropy(empirical_dist))\n",
    "print(\"Frankenstein KL Divergence with Empirical Distribution:\", model_frankenstein.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_wikipedia = load_text('data/wikipedia.txt')\n",
    "tokens_wikipedia = preprocess(text_wikipedia)\n",
    "\n",
    "model_wikipedia = UnigramModel()\n",
    "model_wikipedia.fit(tokens_wikipedia)\n",
    "print(\"Wikipedia Cross-Entropy with Empirical Distribution:\", model_wikipedia.cross_entropy(empirical_dist))\n",
    "print(\"Wikipedia KL Divergence with Empirical Distribution:\", model_wikipedia.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_code = load_text('data/code.txt')\n",
    "tokens_code = preprocess(text_code)\n",
    "\n",
    "model_code = UnigramModel()\n",
    "model_code.fit(tokens_code)\n",
    "print(\"Code Cross-Entropy with Empirical Distribution:\", model_code.cross_entropy(empirical_dist))\n",
    "print(\"Code KL Divergence with Empirical Distribution:\", model_code.kl_divergence(empirical_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e6873",
   "metadata": {},
   "source": [
    "## Text Compression (4 points)\n",
    "\n",
    "Let's say we want to compress our datasets with a prefix-free binary code.\n",
    "\n",
    "1. Write a function that computes the optimal length for each word in a given distribution. [0.5 points]\n",
    "<!-- As explained in the lecture, a nice way of constructing a code would be, is to determine the length of the encoding a token based on the frequency of the token. This can be done in many ways. In the lecture we talked about prefix codes:\n",
    "No code word is a prefix of another code wordWe can organize the code as a tree\n",
    "\n",
    "Given an arbitrary alphabet along with probabilities for each token, you are to implement a function that outputs the encoding for each character. (3 points.)\n",
    "\n",
    "**HINT**: feel free to use the example in the slides to validate that your generated encoding is correct:\n",
    "\n",
    "\n",
    "| word | frequency | $C(\\text{word})$ |\n",
    "|:-----|:----------|:-----------------|\n",
    "| the  |0.5        |`0`               |\n",
    "| and  |0.25       |`10`              |\n",
    "| of   |0.125      |`110`             |\n",
    "| he   |0.125      |`111`             |\n",
    "\n",
    "\n",
    "Where $C(\\text{word})$ represents the encoding of word.\n",
    "\n",
    "Though this algorithm is generalizable to any base of the code (i.e. the code need not be binary), we shall limit this exercise to binary encoding. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_binary_length(distribution : Dict[str, float]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Calculate the optimal binary length for a given distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution: A dictionary of token probabilities.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with tokens as keys and their optimal binary lengths as values.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c129ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal Encoding Length: \\\"if\\\"\")\n",
    "print(f\"Frankenstein:\\t{optimal_binary_length(model_frankenstein.dist)['if']:.4f}\")\n",
    "print(f\"Wikipedia:\\t{optimal_binary_length(model_wikipedia.dist)['if']:.4f}\")\n",
    "print(f\"Code:\\t\\t{optimal_binary_length(model_code.dist)['if']:.4f}\")\n",
    "print()\n",
    "print(\"Optimal Encoding Length: \\\"the\\\"\")\n",
    "print(f\"Frankenstein:\\t{optimal_binary_length(model_frankenstein.dist)['the']:.4f}\")\n",
    "print(f\"Wikipedia:\\t{optimal_binary_length(model_wikipedia.dist)['the']:.4f}\")\n",
    "print(f\"Code:\\t\\t{optimal_binary_length(model_code.dist)['the']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9cace",
   "metadata": {},
   "source": [
    "2. Write a function that returns an expected code length of a token sequence, given a probability distribution. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_code_length(tokens: List[str], distribution: Dict[str, float]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the expected code length for a given distribution.\n",
    "\n",
    "    Args:\n",
    "        distribution: A dictionary of token probabilities.\n",
    "    Returns:\n",
    "        float: The expected code length.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_length_frankenstein = expected_code_length(tokens_frankenstein, model_frankenstein.dist)\n",
    "expected_length_wikipedia = expected_code_length(tokens_wikipedia, model_wikipedia.dist)\n",
    "expected_length_code = expected_code_length(tokens_code, model_code.dist)\n",
    "\n",
    "print(\"Expected Code Length for Frankenstein:\", expected_length_frankenstein)\n",
    "print(\"Number of tokens in Frankenstein:\", len(tokens_frankenstein))\n",
    "print()\n",
    "print(\"Expected Code Length for Wikipedia:\", expected_length_wikipedia)\n",
    "print(\"Number of tokens in Wikipedia:\", len(tokens_wikipedia))\n",
    "print()\n",
    "print(\"Expected Code Length for Code:\", expected_length_code)\n",
    "print(\"Number of tokens in Code:\", len(tokens_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af1f872",
   "metadata": {},
   "source": [
    "3. Consider four binary codes.\n",
    "\n",
    "| word | $C_1(\\text{word})$| $C_2(\\text{word})$| $C_3(\\text{word})$| $C_4(\\text{word})$|\n",
    "|:-----|:------------------|:------------------|:------------------|:------------------|\n",
    "| the  |`100`              |`0`                |`0`                |`11`               |\n",
    "| and  |`01`               |`10`               |`10`               |`110`              |\n",
    "| of   |`110`              |`110`              |`110`              |`1011`             |\n",
    "| and  |`1110`             |`1110`             |`111`              |`0`                |\n",
    "| to   |`1111`             |`1111`             |`1111`             |`1101`             |\n",
    "\n",
    "* Which of these codes are prefix-free? For other codes, explain why they are not. [0.5 points]\n",
    "\n",
    "* Which of these codes satisfy Kraft's inequality? [0.5 points]\n",
    "\n",
    "4. Prove mathematically that Kraft's inequality holds for all prefix-free binary codes. **HINT**: think about how many leaves there are at a binary tree's depth $l_n$. [2 points]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
