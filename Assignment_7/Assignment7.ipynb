{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf375e76",
   "metadata": {},
   "source": [
    "# SNLP Assignment 7\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk\n",
    "! pip install numpy\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install seaborn\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23a0f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108de26",
   "metadata": {},
   "source": [
    "## Exercise 1: BoW and TF-IDF [4.5 points]\n",
    "\n",
    "### Exercise 1.1\n",
    "\n",
    "Write a function that takes a preprocessed tokenized text corpus and a word2id mapping and turns it into a Bag-of-Words matrix. You would need to generate the mapping yourself in the form of a dictionary (e.g. `{\"the\": 0, \"of\": 1, ...}`) [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8e4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences.\n",
    "    Returns:\n",
    "        tokenized_corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def preprocess_corpus(corpus: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess the given corpus by lowercasing and removing punctuation.\n",
    "    \n",
    "    Args:\n",
    "        corpus: tokenized corpus.\n",
    "    Returns:\n",
    "        preprocessed_corpus - preprocessed corpus.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def token2id(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create a mapping from tokens to unique IDs.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    Returns:\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def bow_matrix(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a bag-of-words representation of the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    Returns:\n",
    "        bag_of_words - Bag-of-words matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24afc5",
   "metadata": {},
   "source": [
    "Now write a function that would include both unigrams and birgams in the BoW matrix. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_with_bigrams(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create a mapping from tokens to unique IDs, with bigrams.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    Returns:\n",
    "        token2id - Dictionary mapping tokens and bigrams to unique IDs.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def bow_matrix_with_bigrams(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a bag-of-words representation of the given corpus, with bigrams.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens and bigrams to unique IDs.\n",
    "    Returns:\n",
    "        bag_of_words - Bag-of-words matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195907e",
   "metadata": {},
   "source": [
    "Take the Yahoo! Answers corpus and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fd82f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>best_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>What makes friendship click?</td>\n",
       "      <td>How does the spark keep going?</td>\n",
       "      <td>good communication is what does it.  Can you m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why does Zebras have stripes?</td>\n",
       "      <td>What is the purpose or those stripes? Who do t...</td>\n",
       "      <td>this provides camouflage - predator vision is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What did the itsy bitsy sipder climb up?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>waterspout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What is the difference between a Bachelors and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One difference between a Bachelors and a Maste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Why do women get PMS?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Premenstrual syndrome (PMS) is a group of symp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      8                       What makes friendship click?   \n",
       "1      1                      Why does Zebras have stripes?   \n",
       "2      3           What did the itsy bitsy sipder climb up?   \n",
       "3      3  What is the difference between a Bachelors and...   \n",
       "4      2                              Why do women get PMS?   \n",
       "\n",
       "                                            question  \\\n",
       "0                     How does the spark keep going?   \n",
       "1  What is the purpose or those stripes? Who do t...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         best_answer  \n",
       "0  good communication is what does it.  Can you m...  \n",
       "1  this provides camouflage - predator vision is ...  \n",
       "2                                         waterspout  \n",
       "3  One difference between a Bachelors and a Maste...  \n",
       "4  Premenstrual syndrome (PMS) is a group of symp...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/yahoo.csv')\n",
    "id2label = {\n",
    "    0: \"Society & Culture\",\n",
    "    1: \"Science & Mathematics\",\n",
    "    2: \"Health\",\n",
    "    3: \"Education & Reference\",\n",
    "    4: \"Computers & Internet\",\n",
    "    5: \"Sports\",\n",
    "    6: \"Business & Finance\",\n",
    "    7: \"Entertainment & Music\",\n",
    "    8: \"Family & Relationships\",\n",
    "    9: \"Politics & Government\"\n",
    "}\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e85615",
   "metadata": {},
   "source": [
    "Create and preprocess a text corpus, consisting of the question title and body (ignore answers). If the title/body is empty, add an empty string (so, if the question has the title \"How to quit smoking?\" and the body is empty, the resulting question would be \"How to quit smoking?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cb6596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70226c9b",
   "metadata": {},
   "source": [
    "Create a BoW matrix out of the corpus.\n",
    "\n",
    "Using the matrix, find the ids of questions which contain the word \"heart\" and print out the total count of each label assigned to them. Then look at the questions which contain the bigram \"heart attack\" and the corresponding labels.\n",
    "Do the same with another word and bigram of your choosing. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the matrix\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "preprocessed_corpus = preprocess_corpus(tokenized_corpus)\n",
    "token2id_dict = token2id_with_bigrams(preprocessed_corpus)\n",
    "bow = bow_matrix_with_bigrams(preprocessed_corpus, token2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b603aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the labels for \"heart\"\n",
    "\n",
    "# for \"heart attack\"\n",
    "\n",
    "# for unigram and bigram of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301acfa7",
   "metadata": {},
   "source": [
    "### Exercise 1.2: TF-IDF\n",
    "\n",
    "TF-IDF is a metric that is calculated for each term $t$ in each document $d$ with the following formula:\n",
    "\n",
    "$$\\text{TF-IDF}_{t, d} = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "where $\\text{TF}(t, d)$ is the *term frequency*, i.e. number of times term $t$ appears in document $d$ and $\\text{IDF}(t)$ is the *inverse document frequency*, which is defined as follows:\n",
    "\n",
    "$$\\text{IDF}(t) = \\log{\\frac{N}{1 + \\text{df}_{t}}}$$\n",
    "\n",
    "where $N$ is the total number of documents and $\\text{df}_{t}$ is the number of documents containing $t$.\n",
    "\n",
    "Now compute a TF-IDF matrix of the corpus of the shape $\\text{number of documents} \\times \\text{number of terms}$. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF representation of the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    Returns:\n",
    "        tfidf_matrix - TF-IDF matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca08c7",
   "metadata": {},
   "source": [
    "In your own words, what role could IDF play in TF-IDF? [0.5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f287f05",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Stop Words\n",
    "\n",
    "Use nltk's stop words (`nltk.corpus.stopwords.words('english')`) to calculate the amount of stop words in the corpus (number of stop words divided by total number of words in the whole corpus). What does the result tell you about the data? [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e1deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_rate(corpus: List[List[str]], stopwords: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the rate of stopwords in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        stopwords - List of stopwords.\n",
    "    Returns:\n",
    "        stopword_rate - Rate of stopwords in the corpus.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb94850",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_rate(preprocessed_corpus, nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ff345",
   "metadata": {},
   "source": [
    "## Exercise 2: Classifiers [3.5 points]\n",
    "\n",
    "### Exercise 2.1\n",
    "\n",
    "1. What is the difference between classification and clustering? Describe with an example of a text dataset. [0.5 points]\n",
    "2. Name one text classification task that you encounter in your day-to-day life. [0.25 points]\n",
    "3. Provide an pair of examples for datasets that are suitable for:\n",
    "    - Binary classification and multi-class classification. [0.25 points]\n",
    "    - Flat classification and hierarchical classification. [0.25 points]\n",
    "    - Single-category classification and multi-category classification [0.25 points]\n",
    "\n",
    "### Exercise 2.2\n",
    "\n",
    "Load the dataset of tweets from the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63bf80c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population (2020)</th>\n",
       "      <th>Land Area (Km)</th>\n",
       "      <th>Density (P/Km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population (2020)  Land Area (Km)  Density (P/Km)  \n",
       "0  Afghanistan           38928346        652860.0              60  \n",
       "1      Albania            2877797         27400.0             105  \n",
       "2      Algeria           43851044       2381740.0              18  \n",
       "3      Andorra              77265           470.0             164  \n",
       "4       Angola           32866272       1246700.0              26  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = pd.read_csv('data/twitter.csv')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d70cd2",
   "metadata": {},
   "source": [
    "Use `LabelEncoder` to turn the `sentiment` columns into numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb6c839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2c903",
   "metadata": {},
   "source": [
    "Split the dataset into train and test data using `train_test_split` by `scikit-learn` (80/20 split). Use the `sentiment` label as the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffd2e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b22b6",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` of `scikit-learn` to create the following 5 bag-of-words matrixes, passing the parameter `ngram_range`. Don't worry about lowercasing and tokenizing, since the CountVectorizer has these functionalities already built-in. Checking out the documentation is highly recommended! [0.5 points]\n",
    "\n",
    "- uni-gram\n",
    "- bi-gram\n",
    "- tri-gram\n",
    "- uni-gram & bi-gram\n",
    "- uni-gram & bi-gram & tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f02573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a45295",
   "metadata": {},
   "source": [
    "\n",
    "Train a kNN classifier on the five matrices. Print out the resulting accuracies on the test data. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6370ebea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd292d",
   "metadata": {},
   "source": [
    "Train a Random Forest classifier on your matrices from Exercise 1.1 and 1.2. Print out the resulting accuracies. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c174ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a66fa0",
   "metadata": {},
   "source": [
    "Train a Naive Bayes classifier on your matrices from Exercise 1.1 and 1.2. Print out the resulting accuracy. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a35a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6690c",
   "metadata": {},
   "source": [
    "## Exercise 3: Pointwise Mutual Information [2 points]\n",
    "\n",
    "Write a function to calculate word frequencies for each class and store the counts of each word within each class. Then, implement a function to compute PMI for each word per class. We are interested in identifying the words most strongly correlated with each class. Print the top 20 features (words) for each class. Use the twitter dataset. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17968c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculate word frequencies for each class on train data\n",
    "def calculate_word_frequencies(X: List[List[str]], y: List[int], num_labels: int = 3) -> Tuple[Dict[str, List[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Calculate word frequencies for each class in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        X - List of tokenized sentences.\n",
    "        y - List of labels corresponding to the sentences.\n",
    "        num_labels - Number of unique labels in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        word_counts - Dictionary mapping words to their frequency counts for each class.\n",
    "        class_counts - List of counts for each class.\"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def compute_pmi(word_counts: Dict[str, List[int]], class_counts: List[int]) -> Dict[Tuple[str, int], float]:\n",
    "    \"\"\"\n",
    "    Compute Pointwise Mutual Information (PMI) for each word in each class.\n",
    "    \n",
    "    Args:\n",
    "        word_counts - Dictionary mapping words to their frequency counts for each class.\n",
    "        class_counts - List of counts for each class.\n",
    "        \n",
    "    Returns:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI score.\"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Print top features (words) for each class based on PMI\n",
    "def top_n_features(pmi: Dict[Tuple[str, int], float], label_mapping: Dict[int, str], top_n: int = 100) -> Dict[int, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Get the top N features (words) for each class based on PMI scores.\n",
    "    \n",
    "    Args:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI score.\n",
    "        label_mapping - Dictionary mapping class indices to class labels.\n",
    "        top_n - Number of top features to return for each class.\n",
    "        \n",
    "    Returns:\n",
    "        top_features - Dictionary mapping class indices to lists of top N features (words) and their PMI scores.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Tokenize, preprocess, and split the Twitter dataset\n",
    "\n",
    "# Calculate word frequencies and PMI for the Twitter dataset\n",
    "\n",
    "# Print top features for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a75a8",
   "metadata": {},
   "source": [
    "Train a decision tree classifier on the top N best words from PMI (regardless of class). Use the `CountVectorizer` to force the train and test sets into this new vocabulary of the best N words. Plot your results, showing how the accuracy behaves with different sizes of the vocabulary.\n",
    "You can use `N_values = np.linspace(10, num_unique_words, 20, dtype=int)` for some nice N values for your top features vocabulary. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f562b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features_overall(pmi: Dict[Tuple[str, int], float], N: int) -> List[str]:\n",
    "    \"\"\"Get top N features based on overall PMI scores.\n",
    "\n",
    "    Args:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI scores.\n",
    "        N - Number of top features to return.\n",
    "    Returns:\n",
    "        top_features - List of top N features based on overall PMI scores.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Determine number of unique words\n",
    "\n",
    "# Define intervals for N\n",
    "\n",
    "# Train decision tree classifier for each value of N\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9036bf",
   "metadata": {},
   "source": [
    "## Bonus: Neural Classifier [2 points]\n",
    "\n",
    "In this exercise, we are going to use a classifier based on DistilBERT, which is a smaller, faster, cheaper and lighter version of BERT.\n",
    "\n",
    "A general recommendation here is to run this on [Google Colab](https://colab.research.google.com/) or [Kaggle](https://www.kaggle.com/), unless you have a relatively powerful computer and you know what you are doing.\n",
    "\n",
    "You might have to install a few requirements by running `!pip install <package>` if you get errors.\n",
    "\n",
    "In the following cell, you will find everything you need in order to train your classifier, but a couple of lines have been erased for you to fill in.\n",
    "\n",
    "You will be working with the [FinancialPhraseBank](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news) dataset, which you can find in the data folder under `data/finance.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cabcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here!\n",
    "#\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# create 2 pandas dataframes train_df and test_df\n",
    "# train_df holds the sentences of X_train and the labels y_train (\"sentence\": X_train, \"label\": y_train)\n",
    "# test_df is analogous for the test set\n",
    "train_df = _\n",
    "test_df = _\n",
    "\n",
    "# now convert the pandas DataFrames into HuggingFace Dataset objects\n",
    "train_dataset = _\n",
    "test_dataset = _\n",
    "\n",
    "# import the distilbert-base-uncased tokenizer using .from_pretrained\n",
    "tokenizer = _\n",
    "\n",
    "# write your tokenize function that passes a sentence from the data to the tokenizer\n",
    "def tokenize_function(data):\n",
    "    return tokenizer( _ , truncation=True)\n",
    "\n",
    "# now call the .map() function of your both huggingface datasets from above and pass the\n",
    "# tokenize_function, but without parentheses. Also pass batched=True\n",
    "train_dataset = _\n",
    "test_dataset = _\n",
    "\n",
    "# now call the .set_format() function of your datasets and set the format to type='torch' \n",
    "# and columns=['input_ids', 'attention_mask', 'label']\n",
    "train_dataset.set_format(_, _)\n",
    "test_dataset.set_format(_, _)\n",
    "\n",
    "# load distilbert-base-uncased as a model for sequence classification and set the correct number of labels\n",
    "model = _\n",
    "\n",
    "# Simply use these predefined arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# use this data collator \n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Get predictions\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "preds = preds_output.predictions.argmax(-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
