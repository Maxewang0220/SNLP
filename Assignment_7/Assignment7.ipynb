{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf375e76",
   "metadata": {},
   "source": [
    "# SNLP Assignment 7\n",
    "\n",
    "Name 1: Entang Wang<br>\n",
    "Student id 1: 7069521<br>\n",
    "Email 1: enwa00001@stud.uni-saarland.de<br>\n",
    "\n",
    "Name 2: Zichao Wei<br>\n",
    "Student id 2: 7063941<br>\n",
    "Email 2: ziwe00001@stud.uni-saarland.de<br>\n",
    "\n",
    "Name 3: Xiao Wang<br>\n",
    "Student id 3: 7039023<br>\n",
    "Email 3: xiwa00004@stud.uni-saarland.de<br>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bbb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Label\n",
    "! pip install nltk\n",
    "! pip install numpy\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install seaborn\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a0f911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:48:39.125217Z",
     "start_time": "2025-06-02T14:48:37.309429Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a108de26",
   "metadata": {},
   "source": [
    "## Exercise 1: BoW and TF-IDF [4.5 points]\n",
    "\n",
    "### Exercise 1.1\n",
    "\n",
    "Write a function that takes a preprocessed tokenized text corpus and a word2id mapping and turns it into a Bag-of-Words matrix. You would need to generate the mapping yourself in the form of a dictionary (e.g. `{\"the\": 0, \"of\": 1, ...}`) [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8e4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenize the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences.\n",
    "    Returns:\n",
    "        tokenized_corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    tokenized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        tokens = sentence.split()\n",
    "        tokenized_corpus.append(tokens)\n",
    "    return tokenized_corpus\n",
    "\n",
    "def preprocess_corpus(corpus: List[List[str]]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess the given corpus by lowercasing and removing punctuation.\n",
    "    \n",
    "    Args:\n",
    "        corpus: tokenized corpus.\n",
    "    Returns:\n",
    "        preprocessed_corpus - preprocessed corpus.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    preprocessed_corpus = []\n",
    "    for sentence in corpus:\n",
    "        preprocessed_sentence = []\n",
    "        for token in sentence:\n",
    "            # Convert to lowercase\n",
    "            token = token.lower()\n",
    "            # Remove punctuation\n",
    "            token = token.translate(str.maketrans('', '', punctuation))\n",
    "            if token:\n",
    "                preprocessed_sentence.append(token)\n",
    "        preprocessed_corpus.append(preprocessed_sentence)\n",
    "    return preprocessed_corpus\n",
    "\n",
    "def token2id(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create a mapping from tokens to unique IDs.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    Returns:\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    # Collect all unique tokens from the corpus\n",
    "    token_set = set()\n",
    "    for sentence in corpus:\n",
    "        for token in sentence:\n",
    "            token_set.add(token)\n",
    "    \n",
    "    token_to_id = {}\n",
    "    for i, token in enumerate(sorted(token_set)):  \n",
    "        token_to_id[token] = i\n",
    "    \n",
    "    return token_to_id\n",
    "\n",
    "def bow_matrix(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a bag-of-words representation of the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    Returns:\n",
    "        bag_of_words - Bag-of-words matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    num_documents = len(corpus)\n",
    "    vocab_size = len(token2id)\n",
    "    bow_matrix = np.zeros((num_documents, vocab_size), dtype=int)\n",
    "    \n",
    "    for doc_idx, sentence in enumerate(corpus):\n",
    "        for token in sentence:\n",
    "            if token in token2id: \n",
    "                token_idx = token2id[token]\n",
    "                bow_matrix[doc_idx, token_idx] += 1\n",
    "    \n",
    "    return bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24afc5",
   "metadata": {},
   "source": [
    "Now write a function that would include both unigrams and birgams in the BoW matrix. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "307696d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2id_with_bigrams(corpus: List[List[str]]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Create a mapping from tokens to unique IDs, with bigrams.\n",
    "    \n",
    "    Args:\n",
    "        corpus - List of sentences, where each sentence is a list of tokens.\n",
    "    Returns:\n",
    "        token2id - Dictionary mapping tokens and bigrams to unique IDs.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    token_set = set()\n",
    "    # Collect all unique unigrams and bigrams from the corpus\n",
    "    for sentence in corpus:\n",
    "        # Add unigrams\n",
    "        for token in sentence:\n",
    "            token_set.add(token)\n",
    "        \n",
    "        # Add bigrams\n",
    "        for i in range(len(sentence) - 1):\n",
    "            bigram = f\"{sentence[i]}_{sentence[i+1]}\"\n",
    "            token_set.add(bigram)\n",
    "    \n",
    "    token_to_id = {}\n",
    "    for i, token in enumerate(sorted(token_set)):  # Sort for consistency\n",
    "        token_to_id[token] = i\n",
    "    \n",
    "    return token_to_id\n",
    "\n",
    "def bow_matrix_with_bigrams(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a bag-of-words representation of the given corpus, with bigrams.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens and bigrams to unique IDs.\n",
    "    Returns:\n",
    "        bag_of_words - Bag-of-words matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    num_documents = len(corpus)\n",
    "    vocab_size = len(token2id)\n",
    "    bow_matrix = np.zeros((num_documents, vocab_size), dtype=int)\n",
    "    \n",
    "    # Fill the matrix\n",
    "    for doc_idx, sentence in enumerate(corpus):\n",
    "        # Count unigrams\n",
    "        for token in sentence:\n",
    "            if token in token2id:  \n",
    "                token_idx = token2id[token]\n",
    "                bow_matrix[doc_idx, token_idx] += 1\n",
    "        \n",
    "        # Count bigrams\n",
    "        for i in range(len(sentence) - 1):\n",
    "            bigram = f\"{sentence[i]}_{sentence[i+1]}\"\n",
    "            if bigram in token2id: \n",
    "                bigram_idx = token2id[bigram]\n",
    "                bow_matrix[doc_idx, bigram_idx] += 1\n",
    "    \n",
    "    return bow_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195907e",
   "metadata": {},
   "source": [
    "Take the Yahoo! Answers corpus and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fd82f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "best_answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "83fc0e37-bd7e-41a7-a338-9eaf17e10934",
       "rows": [
        [
         "0",
         "0",
         "What's the big deal about Valetine's Day?",
         "I mean , seriously, what's so great about Valentine's Day? Everybody get all lovey-dovey and start acting all stupid and stuff.",
         "Valentine's Day is a big deal because we live in a capitalist country where retail stores make a fortune off getting people to buy useless crap through advertising. Then they bill it as a holiday so that you are a bad person if you criticize it according to them. Like how at Christmas if you aren't willing to go in debt buying everybody stuff you are called Scrooge or how if you don't shell out dough for July 4'th you aren't patriotic etc. etc.",
         "What's the big deal about Valetine's Day? I mean , seriously, what's so great about Valentine's Day? Everybody get all lovey-dovey and start acting all stupid and stuff."
        ],
        [
         "1",
         "0",
         "What are some Native American Manners?",
         null,
         "This question is way to general. Every region has it's own unique culture. Some were war like other more peacful, the maya had cities more vast then any in the western world.",
         "What are some Native American Manners? "
        ],
        [
         "2",
         "0",
         "Why did the Pastor said every Deities like Buddha and hindu gods are all spirits and demon?",
         "Is Buddha statue consist of demon inside",
         "Because he's ignorant!",
         "Why did the Pastor said every Deities like Buddha and hindu gods are all spirits and demon? Is Buddha statue consist of demon inside"
        ],
        [
         "3",
         "0",
         "can ahuman forgive the mistakes for another ahuman ? if so who forgive for the firest one?",
         "why we don't go directly to our god ???\\nif u don't bay u 'll go to hell is that resonable???",
         "Yes and no. I can forgive people for things they've done to me, and they can do likewise. \\n\\nBut if you're asking whether humans can take away someone's guilt by forgiving them, then no. When we sin against someone, we also sin against God, because we're rebelling against the way He made us. So only God can forgive us and take away our guilt.\\n\\nIs this what you were asking? If not, let me know.",
         "can ahuman forgive the mistakes for another ahuman ? if so who forgive for the firest one? why we don't go directly to our god ???\\nif u don't bay u 'll go to hell is that resonable???"
        ],
        [
         "4",
         "0",
         "Who is Statira?",
         null,
         "Daughter of Darius III of Persia",
         "Who is Statira? "
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>best_answer</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What's the big deal about Valetine's Day?</td>\n",
       "      <td>I mean , seriously, what's so great about Vale...</td>\n",
       "      <td>Valentine's Day is a big deal because we live ...</td>\n",
       "      <td>What's the big deal about Valetine's Day? I me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>What are some Native American Manners?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This question is way to general. Every region ...</td>\n",
       "      <td>What are some Native American Manners?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the Pastor said every Deities like Bud...</td>\n",
       "      <td>Is Buddha statue consist of demon inside</td>\n",
       "      <td>Because he's ignorant!</td>\n",
       "      <td>Why did the Pastor said every Deities like Bud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>can ahuman forgive the mistakes for another ah...</td>\n",
       "      <td>why we don't go directly to our god ???\\nif u ...</td>\n",
       "      <td>Yes and no. I can forgive people for things th...</td>\n",
       "      <td>can ahuman forgive the mistakes for another ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Who is Statira?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Daughter of Darius III of Persia</td>\n",
       "      <td>Who is Statira?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      0          What's the big deal about Valetine's Day?   \n",
       "1      0             What are some Native American Manners?   \n",
       "2      0  Why did the Pastor said every Deities like Bud...   \n",
       "3      0  can ahuman forgive the mistakes for another ah...   \n",
       "4      0                                    Who is Statira?   \n",
       "\n",
       "                                            question  \\\n",
       "0  I mean , seriously, what's so great about Vale...   \n",
       "1                                                NaN   \n",
       "2           Is Buddha statue consist of demon inside   \n",
       "3  why we don't go directly to our god ???\\nif u ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                         best_answer  \\\n",
       "0  Valentine's Day is a big deal because we live ...   \n",
       "1  This question is way to general. Every region ...   \n",
       "2                             Because he's ignorant!   \n",
       "3  Yes and no. I can forgive people for things th...   \n",
       "4                   Daughter of Darius III of Persia   \n",
       "\n",
       "                                                text  \n",
       "0  What's the big deal about Valetine's Day? I me...  \n",
       "1            What are some Native American Manners?   \n",
       "2  Why did the Pastor said every Deities like Bud...  \n",
       "3  can ahuman forgive the mistakes for another ah...  \n",
       "4                                   Who is Statira?   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/yahoo.csv')\n",
    "id2label = {\n",
    "    0: \"Society & Culture\",\n",
    "    1: \"Science & Mathematics\",\n",
    "    2: \"Health\",\n",
    "    3: \"Education & Reference\",\n",
    "    4: \"Computers & Internet\",\n",
    "    5: \"Sports\",\n",
    "    6: \"Business & Finance\",\n",
    "    7: \"Entertainment & Music\",\n",
    "    8: \"Family & Relationships\",\n",
    "    9: \"Politics & Government\"\n",
    "}\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e85615",
   "metadata": {},
   "source": [
    "Create and preprocess a text corpus, consisting of the question title and body (ignore answers). If the title/body is empty, add an empty string (so, if the question has the title \"How to quit smoking?\" and the body is empty, the resulting question would be \"How to quit smoking?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb6596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "def create_corpus(df):\n",
    "    corpus = []\n",
    "    for idx, row in df.iterrows():\n",
    "        title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
    "        question = str(row['question']) if pd.notna(row['question']) else \"\"\n",
    "        \n",
    "        combined_text = f\"{title} {question}\".strip()\n",
    "        if combined_text.lower() in ['nan nan', 'nan', '']:\n",
    "            combined_text = \"\"\n",
    "            \n",
    "        corpus.append(combined_text)\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "corpus = create_corpus(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70226c9b",
   "metadata": {},
   "source": [
    "Create a BoW matrix out of the corpus.\n",
    "\n",
    "Using the matrix, find the ids of questions which contain the word \"heart\" and print out the total count of each label assigned to them. Then look at the questions which contain the bigram \"heart attack\" and the corresponding labels.\n",
    "Do the same with another word and bigram of your choosing. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5532afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the matrix\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "preprocessed_corpus = preprocess_corpus(tokenized_corpus)\n",
    "token2id_dict = token2id_with_bigrams(preprocessed_corpus)\n",
    "bow = bow_matrix_with_bigrams(preprocessed_corpus, token2id_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b603aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing 'heart': 3 total\n",
      "  Health: 1 documents\n",
      "  Family & Relationships: 2 documents\n",
      "'heart_attack' not found in vocabulary\n",
      "Documents containing 'good': 22 total\n",
      "  Society & Culture: 3 documents\n",
      "  Science & Mathematics: 1 documents\n",
      "  Health: 3 documents\n",
      "  Education & Reference: 4 documents\n",
      "  Sports: 1 documents\n",
      "  Business & Finance: 3 documents\n",
      "  Entertainment & Music: 1 documents\n",
      "  Family & Relationships: 5 documents\n",
      "  Politics & Government: 1 documents\n",
      "'good_day' not found in vocabulary\n"
     ]
    }
   ],
   "source": [
    "def analyze_term_labels(term, bow_matrix, token2id_dict, df, id2label):\n",
    "    if term in token2id_dict:\n",
    "        term_idx = token2id_dict[term]\n",
    "        term_doc_indices = np.where(bow_matrix[:, term_idx] > 0)[0]\n",
    "        term_labels = df['label'].iloc[term_doc_indices]\n",
    "        term_label_counts = term_labels.value_counts().sort_index()\n",
    "        \n",
    "        print(f\"Documents containing '{term}': {len(term_doc_indices)} total\")\n",
    "        for label_id, count in term_label_counts.items():\n",
    "            print(f\"  {id2label[label_id]}: {count} documents\")\n",
    "    else:\n",
    "        print(f\"'{term}' not found in vocabulary\")\n",
    "# counting the labels for \"heart\"\n",
    "analyze_term_labels(\"heart\", bow, token2id_dict, df, id2label)\n",
    "# for \"heart attack\"\n",
    "analyze_term_labels(\"heart_attack\", bow, token2id_dict, df, id2label)\n",
    "# for unigram and bigram of your choice\n",
    "unigram = \"good\"\n",
    "bigram = \"good_day\"\n",
    "analyze_term_labels(unigram, bow, token2id_dict, df, id2label)\n",
    "analyze_term_labels(bigram, bow, token2id_dict, df, id2label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301acfa7",
   "metadata": {},
   "source": [
    "### Exercise 1.2: TF-IDF\n",
    "\n",
    "TF-IDF is a metric that is calculated for each term $t$ in each document $d$ with the following formula:\n",
    "\n",
    "$$\\text{TF-IDF}_{t, d} = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "where $\\text{TF}(t, d)$ is the *term frequency*, i.e. number of times term $t$ appears in document $d$ and $\\text{IDF}(t)$ is the *inverse document frequency*, which is defined as follows:\n",
    "\n",
    "$$\\text{IDF}(t) = \\log{\\frac{N}{1 + \\text{df}_{t}}}$$\n",
    "\n",
    "where $N$ is the total number of documents and $\\text{df}_{t}$ is the number of documents containing $t$.\n",
    "\n",
    "Now compute a TF-IDF matrix of the corpus of the shape $\\text{number of documents} \\times \\text{number of terms}$. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d8774f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(corpus: List[List[str]], token2id: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the TF-IDF representation of the given corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        token2id - Dictionary mapping tokens to unique IDs.\n",
    "    Returns:\n",
    "        tfidf_matrix - TF-IDF matrix (document_size x vocab_size).\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    num_documents = len(corpus)\n",
    "    vocab_size = len(token2id)\n",
    "    \n",
    "    # Create Term Frequency matrix\n",
    "    tf_matrix = np.zeros((num_documents, vocab_size), dtype=float)\n",
    "    for doc_idx, document in enumerate(corpus):\n",
    "        for token in document:\n",
    "            if token in token2id:\n",
    "                token_idx = token2id[token]\n",
    "                tf_matrix[doc_idx, token_idx] += 1\n",
    "\n",
    "    # Calculate Document Frequency for each term\n",
    "    df = np.zeros(vocab_size)\n",
    "    for token, token_idx in token2id.items():\n",
    "        # Count how many documents contain this token\n",
    "        documents_with_token = np.sum(tf_matrix[:, token_idx] > 0)\n",
    "        df[token_idx] = documents_with_token\n",
    "    \n",
    "    # Calculate IDF (Inverse Document Frequency) for each term\n",
    "    idf = np.log(num_documents / (1 + df))\n",
    "    \n",
    "    # Calculate TF-IDF matrix\n",
    "    tfidf_matrix = tf_matrix * idf\n",
    "    \n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca08c7",
   "metadata": {},
   "source": [
    "In your own words, what role could IDF play in TF-IDF? [0.5 points]\n",
    "\n",
    "Answer: Some words like 'is', 'the', ... are the common words in a document, but they can't represent the topic the the document. Since they're universal in each document. With IDF term, we can add more values to the words that are unique in the document which is more relevant to the document topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f287f05",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Stop Words\n",
    "\n",
    "Use nltk's stop words (`nltk.corpus.stopwords.words('english')`) to calculate the amount of stop words in the corpus (number of stop words divided by total number of words in the whole corpus). What does the result tell you about the data? [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1deff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_rate(corpus: List[List[str]], stopwords: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the rate of stopwords in the corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus - Tokenized corpus.\n",
    "        stopwords - List of stopwords.\n",
    "    Returns:\n",
    "        stopword_rate - Rate of stopwords in the corpus.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    stopwords_set = set(stopwords)\n",
    "    total_words = 0\n",
    "    stopword_count = 0\n",
    "    \n",
    "    for document in corpus:\n",
    "        for word in document:\n",
    "            total_words += 1\n",
    "            if word in stopwords_set:\n",
    "                stopword_count += 1\n",
    "    \n",
    "    if total_words == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return stopword_count / total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdb94850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49670218163368846"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_rate(preprocessed_corpus, nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ff345",
   "metadata": {},
   "source": [
    "## Exercise 2: Classifiers [3.5 points]\n",
    "\n",
    "### Exercise 2.1\n",
    "\n",
    "1. What is the difference between classification and clustering? Describe with an example of a text dataset. [0.5 points]\n",
    "2. Name one text classification task that you encounter in your day-to-day life. [0.25 points]\n",
    "3. Provide an pair of examples for datasets that are suitable for:\n",
    "    - Binary classification and multi-class classification. [0.25 points]\n",
    "    - Flat classification and hierarchical classification. [0.25 points]\n",
    "    - Single-category classification and multi-category classification [0.25 points]\n",
    "\n",
    "### Answer:\n",
    "1. Classification is a supervised task in which each text comes with a known label and the model learns to assign those predefined labels to unseen texts, e.g. IMDB movie reviews, while clustering is an unsupervised task that discovers its own groupings among texts that arrive with no labels, e.g. BBC news. \n",
    "\n",
    "2. E-mail spam filtering. The incoming emails are automatically classified as spam or inbox. These filters rely on supervised models trained on enormous corpora of previously labelled e-mail.\n",
    "\n",
    "3. \n",
    "- Binary: IMDB Movie Reviews Multi-class: 20 Newsgroups\n",
    "- Flat: 20 Newsgroups Hierarchical: RCV1-v2\n",
    "- Single-category: IMDB Movie Reviews Multi-category: Reuters-21578\n",
    "### Exercise 2.2\n",
    "\n",
    "Load the dataset of tweets from the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63bf80c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:48:47.329578Z",
     "start_time": "2025-06-02T14:48:47.293173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "textID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "selected_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Time of Tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age of User",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Population (2020)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Land Area (Km)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Density (P/Km)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e205a7d7-6e52-45c7-a7ec-6963f0d4808a",
       "rows": [
        [
         "0",
         "32883c8788",
         "wishes the rain would stop so my stupid headache would go away!",
         "wishes the rain would stop so my stupid headache would go away!",
         "negative",
         "night",
         "70-100",
         "Croatia",
         "4105267",
         "55960.0",
         "73",
         "0"
        ],
        [
         "1",
         "899ba63056",
         " Sorry to disappoint. Not a big Nascar fan but I`m still an all-around decent redneck other than that.",
         "Sorry",
         "negative",
         "noon",
         "60-70",
         "Colombia",
         "50882891",
         "1109500.0",
         "46",
         "0"
        ],
        [
         "2",
         "d31f708485",
         "playing singstar without my fave duetter",
         "playing singstar without my fave duetter",
         "negative",
         "night",
         "70-100",
         "India",
         "1380004385",
         "2973190.0",
         "464",
         "0"
        ],
        [
         "3",
         "cea2861940",
         "spray tan = fail on legs and feet. I`ve been scrubbing them and feet look better, but they looked awful this morning  Everywhere else = ok",
         "spray tan = fail on legs and feet. I`ve been scrubbing them and feet look better, but they looked awful this morning  Everywhere else = ok",
         "negative",
         "noon",
         "60-70",
         "Belgium",
         "11589623",
         "30280.0",
         "383",
         "0"
        ],
        [
         "4",
         "52baecd545",
         ": first impression is that it`s considerably  slower to boot than 2008  #beta1",
         "first impression is that it`s considerably  slower",
         "negative",
         "night",
         "31-45",
         "Australia",
         "25499884",
         "7682300.0",
         "3",
         "0"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population (2020)</th>\n",
       "      <th>Land Area (Km)</th>\n",
       "      <th>Density (P/Km)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32883c8788</td>\n",
       "      <td>wishes the rain would stop so my stupid headac...</td>\n",
       "      <td>wishes the rain would stop so my stupid headac...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>4105267</td>\n",
       "      <td>55960.0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>899ba63056</td>\n",
       "      <td>Sorry to disappoint. Not a big Nascar fan but...</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>50882891</td>\n",
       "      <td>1109500.0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d31f708485</td>\n",
       "      <td>playing singstar without my fave duetter</td>\n",
       "      <td>playing singstar without my fave duetter</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>India</td>\n",
       "      <td>1380004385</td>\n",
       "      <td>2973190.0</td>\n",
       "      <td>464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cea2861940</td>\n",
       "      <td>spray tan = fail on legs and feet. I`ve been s...</td>\n",
       "      <td>spray tan = fail on legs and feet. I`ve been s...</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>11589623</td>\n",
       "      <td>30280.0</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52baecd545</td>\n",
       "      <td>: first impression is that it`s considerably  ...</td>\n",
       "      <td>first impression is that it`s considerably  sl...</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  32883c8788  wishes the rain would stop so my stupid headac...   \n",
       "1  899ba63056   Sorry to disappoint. Not a big Nascar fan but...   \n",
       "2  d31f708485           playing singstar without my fave duetter   \n",
       "3  cea2861940  spray tan = fail on legs and feet. I`ve been s...   \n",
       "4  52baecd545  : first impression is that it`s considerably  ...   \n",
       "\n",
       "                                       selected_text sentiment Time of Tweet  \\\n",
       "0  wishes the rain would stop so my stupid headac...  negative         night   \n",
       "1                                              Sorry  negative          noon   \n",
       "2           playing singstar without my fave duetter  negative         night   \n",
       "3  spray tan = fail on legs and feet. I`ve been s...  negative          noon   \n",
       "4  first impression is that it`s considerably  sl...  negative         night   \n",
       "\n",
       "  Age of User    Country  Population (2020)  Land Area (Km)  Density (P/Km)  \\\n",
       "0      70-100    Croatia            4105267         55960.0              73   \n",
       "1       60-70   Colombia           50882891       1109500.0              46   \n",
       "2      70-100      India         1380004385       2973190.0             464   \n",
       "3       60-70    Belgium           11589623         30280.0             383   \n",
       "4       31-45  Australia           25499884       7682300.0               3   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df = pd.read_csv('data/twitter.csv')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d70cd2",
   "metadata": {},
   "source": [
    "Use `LabelEncoder` to turn the `sentiment` columns into numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eb6c839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:48:53.210405Z",
     "start_time": "2025-06-02T14:48:53.201405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "textID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "selected_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Time of Tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Age of User",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Population (2020)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Land Area (Km)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Density (P/Km)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "9872a534-36fc-4bba-8236-66d8d5af21dd",
       "rows": [
        [
         "0",
         "32883c8788",
         "wishes the rain would stop so my stupid headache would go away!",
         "wishes the rain would stop so my stupid headache would go away!",
         "0",
         "night",
         "70-100",
         "Croatia",
         "4105267",
         "55960.0",
         "73",
         "0"
        ],
        [
         "1",
         "899ba63056",
         " Sorry to disappoint. Not a big Nascar fan but I`m still an all-around decent redneck other than that.",
         "Sorry",
         "0",
         "noon",
         "60-70",
         "Colombia",
         "50882891",
         "1109500.0",
         "46",
         "0"
        ],
        [
         "2",
         "d31f708485",
         "playing singstar without my fave duetter",
         "playing singstar without my fave duetter",
         "0",
         "night",
         "70-100",
         "India",
         "1380004385",
         "2973190.0",
         "464",
         "0"
        ],
        [
         "3",
         "cea2861940",
         "spray tan = fail on legs and feet. I`ve been scrubbing them and feet look better, but they looked awful this morning  Everywhere else = ok",
         "spray tan = fail on legs and feet. I`ve been scrubbing them and feet look better, but they looked awful this morning  Everywhere else = ok",
         "0",
         "noon",
         "60-70",
         "Belgium",
         "11589623",
         "30280.0",
         "383",
         "0"
        ],
        [
         "4",
         "52baecd545",
         ": first impression is that it`s considerably  slower to boot than 2008  #beta1",
         "first impression is that it`s considerably  slower",
         "0",
         "night",
         "31-45",
         "Australia",
         "25499884",
         "7682300.0",
         "3",
         "0"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population (2020)</th>\n",
       "      <th>Land Area (Km)</th>\n",
       "      <th>Density (P/Km)</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32883c8788</td>\n",
       "      <td>wishes the rain would stop so my stupid headac...</td>\n",
       "      <td>wishes the rain would stop so my stupid headac...</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>4105267</td>\n",
       "      <td>55960.0</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>899ba63056</td>\n",
       "      <td>Sorry to disappoint. Not a big Nascar fan but...</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>0</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>50882891</td>\n",
       "      <td>1109500.0</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d31f708485</td>\n",
       "      <td>playing singstar without my fave duetter</td>\n",
       "      <td>playing singstar without my fave duetter</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>India</td>\n",
       "      <td>1380004385</td>\n",
       "      <td>2973190.0</td>\n",
       "      <td>464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cea2861940</td>\n",
       "      <td>spray tan = fail on legs and feet. I`ve been s...</td>\n",
       "      <td>spray tan = fail on legs and feet. I`ve been s...</td>\n",
       "      <td>0</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>11589623</td>\n",
       "      <td>30280.0</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52baecd545</td>\n",
       "      <td>: first impression is that it`s considerably  ...</td>\n",
       "      <td>first impression is that it`s considerably  sl...</td>\n",
       "      <td>0</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Australia</td>\n",
       "      <td>25499884</td>\n",
       "      <td>7682300.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  32883c8788  wishes the rain would stop so my stupid headac...   \n",
       "1  899ba63056   Sorry to disappoint. Not a big Nascar fan but...   \n",
       "2  d31f708485           playing singstar without my fave duetter   \n",
       "3  cea2861940  spray tan = fail on legs and feet. I`ve been s...   \n",
       "4  52baecd545  : first impression is that it`s considerably  ...   \n",
       "\n",
       "                                       selected_text  sentiment Time of Tweet  \\\n",
       "0  wishes the rain would stop so my stupid headac...          0         night   \n",
       "1                                              Sorry          0          noon   \n",
       "2           playing singstar without my fave duetter          0         night   \n",
       "3  spray tan = fail on legs and feet. I`ve been s...          0          noon   \n",
       "4  first impression is that it`s considerably  sl...          0         night   \n",
       "\n",
       "  Age of User    Country  Population (2020)  Land Area (Km)  Density (P/Km)  \\\n",
       "0      70-100    Croatia            4105267         55960.0              73   \n",
       "1       60-70   Colombia           50882891       1109500.0              46   \n",
       "2      70-100      India         1380004385       2973190.0             464   \n",
       "3       60-70    Belgium           11589623         30280.0             383   \n",
       "4       31-45  Australia           25499884       7682300.0               3   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Your code here!\n",
    "label_encoder = LabelEncoder()\n",
    "tweet_df['sentiment'] = label_encoder.fit_transform(tweet_df['sentiment'])\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c2c903",
   "metadata": {},
   "source": [
    "Split the dataset into train and test data using `train_test_split` by `scikit-learn` (80/20 split). Use the `sentiment` label as the class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffd2e7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:20.668300Z",
     "start_time": "2025-06-02T14:49:20.658290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41            I`m stuck with BOO!!!! Jeeeez shoot me now\n",
      "165                  If I may suggest: http://tr.im/kXkw\n",
      "246    Got the sniffles   I SO don`t want to get sick...\n",
      "179     Um yeah ... role model for your peers you may...\n",
      "192    Missed the UPS guy again! Ugh so sad  But i go...\n",
      "Name: text, dtype: object\n",
      "41     0\n",
      "165    1\n",
      "246    2\n",
      "179    1\n",
      "192    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "X_train_tweet, X_test_tweet, y_train_tweet, y_test_tweet = train_test_split(\n",
    "    tweet_df['text'], tweet_df['sentiment'], test_size=0.2, random_state=42, stratify=tweet_df['sentiment']\n",
    ")\n",
    "print(X_train_tweet[:5])\n",
    "print(y_train_tweet[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b22b6",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` of `scikit-learn` to create the following 5 bag-of-words matrixes, passing the parameter `ngram_range`. Don't worry about lowercasing and tokenizing, since the CountVectorizer has these functionalities already built-in. Checking out the documentation is highly recommended! [0.5 points]\n",
    "\n",
    "- uni-gram\n",
    "- bi-gram\n",
    "- tri-gram\n",
    "- uni-gram & bi-gram\n",
    "- uni-gram & bi-gram & tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19f02573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:30.531387Z",
     "start_time": "2025-06-02T14:49:29.797522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BoW for: unigram with ngram_range=(1, 1)\n",
      "Creating BoW for: bigram with ngram_range=(2, 2)\n",
      "Creating BoW for: trigram with ngram_range=(3, 3)\n",
      "Creating BoW for: unigram_bigram with ngram_range=(1, 2)\n",
      "Creating BoW for: unigram_bigram_trigram with ngram_range=(1, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Your code here!\n",
    "ngram_ranges = {\n",
    "    \"unigram\": (1, 1),\n",
    "    \"bigram\": (2, 2),\n",
    "    \"trigram\": (3, 3),\n",
    "    \"unigram_bigram\": (1, 2),\n",
    "    \"unigram_bigram_trigram\": (1, 3)\n",
    "}\n",
    "\n",
    "vectorizers_tweet = {}\n",
    "X_train_tweet_bow = {}\n",
    "X_test_tweet_bow = {}\n",
    "\n",
    "for name, ngram_range in ngram_ranges.items():\n",
    "    print(f\"Creating BoW for: {name} with ngram_range={ngram_range}\")\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    X_train_tweet_bow[name] = vectorizer.fit_transform(X_train_tweet)\n",
    "    X_test_tweet_bow[name] = vectorizer.transform(X_test_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a45295",
   "metadata": {},
   "source": [
    "\n",
    "Train a kNN classifier on the five matrices. Print out the resulting accuracies on the test data. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6370ebea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:49:58.458150Z",
     "start_time": "2025-06-02T14:49:56.979243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN Accuracies (Twitter data):\n",
      "unigram: 0.3333\n",
      "bigram: 0.3167\n",
      "trigram: 0.3500\n",
      "unigram_bigram: 0.2833\n",
      "unigram_bigram_trigram: 0.3500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Your code here!\n",
    "knn_accuracies_tweet = {}\n",
    "k_neighbors = 5 # A common default value for k\n",
    "\n",
    "for name in ngram_ranges.keys():\n",
    "    knn = KNeighborsClassifier(n_neighbors=k_neighbors)\n",
    "    knn.fit(X_train_tweet_bow[name], y_train_tweet)\n",
    "    \n",
    "    predictions = knn.predict(X_test_tweet_bow[name])\n",
    "    accuracy = accuracy_score(y_test_tweet, predictions)\n",
    "    knn_accuracies_tweet[name] = accuracy\n",
    "    \n",
    "print(\"kNN Accuracies (Twitter data):\")\n",
    "for name, acc in knn_accuracies_tweet.items():\n",
    "    print(f\"{name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd292d",
   "metadata": {},
   "source": [
    "Train a Random Forest classifier on your matrices from Exercise 1.1 and 1.2. Print out the resulting accuracies. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c174ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracies (Yahoo data):\n",
      "Yahoo_BoW_Unigrams: 0.3100\n",
      "Yahoo_BoW_UniBiGrams: 0.2600\n",
      "Yahoo_TFIDF_Unigrams: 0.3100\n",
      "Yahoo_TFIDF_UniBiGrams: 0.3300\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Your code here!\n",
    "# TODO Fill in the matrices from Exercise 1.1 and 1.2\n",
    "token2id_unigram = token2id(preprocessed_corpus)\n",
    "bow_yahoo_unigram = bow_matrix(preprocessed_corpus, token2id_unigram)\n",
    "\n",
    "token2id_unigram_bigram = token2id_with_bigrams(preprocessed_corpus)\n",
    "bow_yahoo_unigram_bigram = bow_matrix_with_bigrams(preprocessed_corpus, token2id_unigram_bigram)\n",
    "\n",
    "tfidf_yahoo_unigram = tf_idf(preprocessed_corpus, token2id_unigram)\n",
    "tfidf_yahoo_unigram_bigram = tf_idf(preprocessed_corpus, token2id_unigram_bigram)\n",
    "\n",
    "yahoo_data_matrices = {\n",
    "    \"Yahoo_BoW_Unigrams\": bow_yahoo_unigram,\n",
    "    \"Yahoo_BoW_UniBiGrams\": bow_yahoo_unigram_bigram,\n",
    "    \"Yahoo_TFIDF_Unigrams\": tfidf_yahoo_unigram,\n",
    "    \"Yahoo_TFIDF_UniBiGrams\": tfidf_yahoo_unigram_bigram\n",
    "}\n",
    "\n",
    "# TODO Make sure the labels are aligned with the matrices\n",
    "yahoo_labels = df['label'].values\n",
    "\n",
    "yahoo_rf_accuracies = {}\n",
    "\n",
    "for name, matrix_data in yahoo_data_matrices.items():\n",
    "    # Splitting Yahoo data\n",
    "    X_train_yahoo, X_test_yahoo, y_train_yahoo, y_test_yahoo = train_test_split(\n",
    "        matrix_data, yahoo_labels, test_size=0.2, random_state=42, stratify=yahoo_labels\n",
    "    )\n",
    "    \n",
    "    # Initialize RandomForest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_classifier.fit(X_train_yahoo, y_train_yahoo)\n",
    "    \n",
    "    predictions_yahoo = rf_classifier.predict(X_test_yahoo)\n",
    "    accuracy_yahoo = accuracy_score(y_test_yahoo, predictions_yahoo)\n",
    "    yahoo_rf_accuracies[name] = accuracy_yahoo\n",
    "    \n",
    "print(\"Random Forest Accuracies (Yahoo data):\")\n",
    "for name, acc in yahoo_rf_accuracies.items():\n",
    "    print(f\"{name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a66fa0",
   "metadata": {},
   "source": [
    "Train a Naive Bayes classifier on your matrices from Exercise 1.1 and 1.2. Print out the resulting accuracy. [0.5 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a35a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Your code here!\n",
    "yahoo_nb_accuracies = {}\n",
    "\n",
    "for name, matrix_data in yahoo_data_matrices.items():\n",
    "    # Splitting Yahoo data\n",
    "    X_train_yahoo, X_test_yahoo, y_train_yahoo, y_test_yahoo = train_test_split(\n",
    "        matrix_data, yahoo_labels_numeric, test_size=0.2, random_state=42, stratify=yahoo_labels_numeric\n",
    "    )\n",
    "    \n",
    "    # Initialize Naive Bayes Classifier\n",
    "    nb_classifier = MultinomialNB()\n",
    "    nb_classifier.fit(X_train_yahoo, y_train_yahoo)\n",
    "    \n",
    "    predictions_yahoo_nb = nb_classifier.predict(X_test_yahoo)\n",
    "    accuracy_yahoo_nb = accuracy_score(y_test_yahoo, predictions_yahoo_nb)\n",
    "    yahoo_nb_accuracies[name] = accuracy_yahoo_nb\n",
    "    \n",
    "print(\"Naive Bayes Accuracies (Yahoo data):\")\n",
    "for name, acc in yahoo_nb_accuracies.items():\n",
    "    print(f\"{name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb6690c",
   "metadata": {},
   "source": [
    "## Exercise 3: Pointwise Mutual Information [2 points]\n",
    "\n",
    "Write a function to calculate word frequencies for each class and store the counts of each word within each class. Then, implement a function to compute PMI for each word per class. We are interested in identifying the words most strongly correlated with each class. Print the top 20 features (words) for each class. Use the twitter dataset. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17968c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculate word frequencies for each class on train data\n",
    "def calculate_word_frequencies(X: List[List[str]], y: List[int], num_labels: int = 3) -> Tuple[Dict[str, List[int]], List[int]]:\n",
    "    \"\"\"\n",
    "    Calculate word frequencies for each class in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        X - List of tokenized sentences.\n",
    "        y - List of labels corresponding to the sentences.\n",
    "        num_labels - Number of unique labels in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        word_counts - Dictionary mapping words to their frequency counts for each class.\n",
    "        class_counts - List of counts for each class.\"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "def compute_pmi(word_counts: Dict[str, List[int]], class_counts: List[int]) -> Dict[Tuple[str, int], float]:\n",
    "    \"\"\"\n",
    "    Compute Pointwise Mutual Information (PMI) for each word in each class.\n",
    "    \n",
    "    Args:\n",
    "        word_counts - Dictionary mapping words to their frequency counts for each class.\n",
    "        class_counts - List of counts for each class.\n",
    "        \n",
    "    Returns:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI score.\"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Print top features (words) for each class based on PMI\n",
    "def top_n_features(pmi: Dict[Tuple[str, int], float], label_mapping: Dict[int, str], top_n: int = 100) -> Dict[int, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Get the top N features (words) for each class based on PMI scores.\n",
    "    \n",
    "    Args:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI score.\n",
    "        label_mapping - Dictionary mapping class indices to class labels.\n",
    "        top_n - Number of top features to return for each class.\n",
    "        \n",
    "    Returns:\n",
    "        top_features - Dictionary mapping class indices to lists of top N features (words) and their PMI scores.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Tokenize, preprocess, and split the Twitter dataset\n",
    "\n",
    "# Calculate word frequencies and PMI for the Twitter dataset\n",
    "\n",
    "# Print top features for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a75a8",
   "metadata": {},
   "source": [
    "Train a decision tree classifier on the top N best words from PMI (regardless of class). Use the `CountVectorizer` to force the train and test sets into this new vocabulary of the best N words. Plot your results, showing how the accuracy behaves with different sizes of the vocabulary.\n",
    "You can use `N_values = np.linspace(10, num_unique_words, 20, dtype=int)` for some nice N values for your top features vocabulary. [1 point]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f562b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_features_overall(pmi: Dict[Tuple[str, int], float], N: int) -> List[str]:\n",
    "    \"\"\"Get top N features based on overall PMI scores.\n",
    "\n",
    "    Args:\n",
    "        pmi - Dictionary mapping (word, class_index) to PMI scores.\n",
    "        N - Number of top features to return.\n",
    "    Returns:\n",
    "        top_features - List of top N features based on overall PMI scores.\n",
    "    \"\"\"\n",
    "    # Your code here!\n",
    "    pass\n",
    "\n",
    "# Determine number of unique words\n",
    "\n",
    "# Define intervals for N\n",
    "\n",
    "# Train decision tree classifier for each value of N\n",
    "\n",
    "# Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9036bf",
   "metadata": {},
   "source": [
    "## Bonus: Neural Classifier [2 points]\n",
    "\n",
    "In this exercise, we are going to use a classifier based on DistilBERT, which is a smaller, faster, cheaper and lighter version of BERT.\n",
    "\n",
    "A general recommendation here is to run this on [Google Colab](https://colab.research.google.com/) or [Kaggle](https://www.kaggle.com/), unless you have a relatively powerful computer and you know what you are doing.\n",
    "\n",
    "You might have to install a few requirements by running `!pip install <package>` if you get errors.\n",
    "\n",
    "In the following cell, you will find everything you need in order to train your classifier, but a couple of lines have been erased for you to fill in.\n",
    "\n",
    "You will be working with the [FinancialPhraseBank](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news) dataset, which you can find in the data folder under `data/finance.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cabcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here!\n",
    "#\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "# create 2 pandas dataframes train_df and test_df\n",
    "# train_df holds the sentences of X_train and the labels y_train (\"sentence\": X_train, \"label\": y_train)\n",
    "# test_df is analogous for the test set\n",
    "train_df = _\n",
    "test_df = _\n",
    "\n",
    "# now convert the pandas DataFrames into HuggingFace Dataset objects\n",
    "train_dataset = _\n",
    "test_dataset = _\n",
    "\n",
    "# import the distilbert-base-uncased tokenizer using .from_pretrained\n",
    "tokenizer = _\n",
    "\n",
    "# write your tokenize function that passes a sentence from the data to the tokenizer\n",
    "def tokenize_function(data):\n",
    "    return tokenizer( _ , truncation=True)\n",
    "\n",
    "# now call the .map() function of your both huggingface datasets from above and pass the\n",
    "# tokenize_function, but without parentheses. Also pass batched=True\n",
    "train_dataset = _\n",
    "test_dataset = _\n",
    "\n",
    "# now call the .set_format() function of your datasets and set the format to type='torch' \n",
    "# and columns=['input_ids', 'attention_mask', 'label']\n",
    "train_dataset.set_format(_, _)\n",
    "test_dataset.set_format(_, _)\n",
    "\n",
    "# load distilbert-base-uncased as a model for sequence classification and set the correct number of labels\n",
    "model = _\n",
    "\n",
    "# Simply use these predefined arguments for the trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# use this data collator \n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Get predictions\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "preds = preds_output.predictions.argmax(-1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
