{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d767ef",
   "metadata": {},
   "source": [
    "# SNLP Assignment 5\n",
    "\n",
    "Name 1: Entang Wang<br/>\n",
    "Student id 1: 7069521<br/>\n",
    "Email 1: enwa00001@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2b723",
   "metadata": {},
   "source": [
    "## Exercise 1 - Back Off! Probabilities Again! (3 points)\n",
    "\n",
    "Consider the toy corpus:\n",
    "\n",
    ">    I study SNLP\\\n",
    ">    study SNLP I\\\n",
    ">    I like to study SNLP\\\n",
    ">    I study SNLP\n",
    "\n",
    "a. State the Vocabulary of the corpus and unigram counts. What is the bigram probability for the sequence \"I like SNLP\" using add-epsilon-discounting with ε = 0.5? (0.75 points)\n",
    "\n",
    "b. Now use a back-off bigram model with absolute discounting (use d=0.75) and a back-off to smoothed unigrams (with the same epsilon ε = 0.5) (0.75 points)\n",
    "\n",
    "c. Compare the result from both techniques. (0.5 points)\n",
    "\n",
    "d. To actually estimate these n-gram probabilities over a text corpus, we use **Maximum Likelihood Estimation (MLE)**. The estimate for the parameters of the MLE is obtained by getting counts from the corpus and then normalising them so they lie between 0 and 1. Consider the formula from the slides. \n",
    "\n",
    "$$P(w_2 | w_1) = \\frac{P(w_1,w_2)}{P(w_1)}$$\n",
    "\n",
    "Using this, state the empirical formula for finding the conditional probability of unigrams $P(w)$, bigrams $P(w_2|w_1)$, and trigrams $P(w_3|w_1,w_2)$ for a corpus of N words. We do not expect any mathematical proof here, but just the formula for finding the conditional probabilities from the words in the corpus using the shown equation as the starting point. (0.5 points)\n",
    "\n",
    "e. Using MLE, compute the probabilities for the following (assume we lowercase the corpus):  (0.5 points)\\\n",
    "(i) $p(snlp|study)$ \\\n",
    "(ii) $p(snlp|i \\ study)$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b6f2da",
   "metadata": {},
   "source": [
    "## Exercise 2 - Good Turing Smoothing (2 points)\n",
    "\n",
    "Consider the following 2 tables\n",
    "\n",
    "<table style=\"display: inline-block; margin-right: 50px;\">\n",
    "  <caption><strong>Good-Turing Count-of-Counts</strong></caption>\n",
    "  <thead>\n",
    "    <tr><th>Count (N(w,h))</th><th>Count of Counts (n_N(w,h))</th></tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>1</td><td>5000</td></tr>\n",
    "    <tr><td>2</td><td>1600</td></tr>\n",
    "    <tr><td>3</td><td>1000</td></tr>\n",
    "    <tr><td>4</td><td>600</td></tr>\n",
    "    <tr><td>5</td><td>300</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<table style=\"display: inline-block;\">\n",
    "  <caption><strong>Bigrams with History \"wine\"</strong></caption>\n",
    "  <thead>\n",
    "    <tr><th>Count (N(w,h))</th><th>Bigram</th></tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>1</td><td>wine drinker</td></tr>\n",
    "    <tr><td>2</td><td>wine lover</td></tr>\n",
    "    <tr><td>3</td><td>wine glass</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "a. What are the discounted counts under Good–Turing discounting for the three given bigrams?\n",
    "\n",
    "b. The amounts from discounting counts are given to a back-off unigram model. Using such a back-off model, what are the\n",
    "probabilities for the following bigrams?\\\n",
    "(i) $p(drinker|wine)$\\\n",
    "(ii) $p(glass|wine)$\\\n",
    "(iii) $p(mug|wine)$\\\n",
    "Note: $p(mug) = 0.015$, $p(drinker)=0.015$, $p(glass)=0.01$ . State any assumptions that you make.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ddacb",
   "metadata": {},
   "source": [
    "## Exercise 3: Cross-Validation (5 points)\n",
    "\n",
    "\n",
    "Imagine you are a linguist in the remote future who just rediscovered a book called \"Bible\". To your disappointment the book is obviously incomplete; all the pages between Genesis and the Apocalypse are torn out, maybe by some late Christian cult. Since you don't know the language of the book you want to build a first language model that you can use if you find any of the lost parts. You digitize the book with your state-of-the art portable digitizer, and then load it into one of your Python notebooks.\n",
    "\n",
    "### 3.1 Baseline\n",
    "\n",
    "* The two corpora are in the text files `genesis.txt` and `apocalypsis.txt`. Load them into the notebook, preprocess them by removing all non-alphabetical characters, and then concatenate them into a single corpus. Split the corpus into a train and a test set, with the test set comprising the _last_ 20% of the corpus. The functions for `preprocess` and `train_test_split_data` should be implemented in `exercise_3.py` (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import exercise_3\n",
    "exercise_3 = reload(exercise_3)\n",
    "\n",
    "genesis_text = \n",
    "apocalypsis_text = \n",
    "\n",
    "# preprocess\n",
    "genesis_preprocessed = \n",
    "apocalypsis_preprocessd = \n",
    "\n",
    "# concatenate\n",
    "corpus = \n",
    "\n",
    "# train, test split\n",
    "train, test = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb4917",
   "metadata": {},
   "source": [
    "* Using the language model class given, we will estimate a trigram language model on the train set and report perplexity on the test set. First, implement the `perplexity` function in `language_model.py` then calculate it using $\\alpha=1$. Does this represent an unbiased estimate of the model's capacity? (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09333483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: trigram LM\n",
    "from language_model import NGramLM\n",
    "\n",
    "N = \n",
    "alpha = \n",
    "\n",
    "LM = \n",
    "perplexity = \n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b76c2a1",
   "metadata": {},
   "source": [
    "### 3.2 Average Perplexity\n",
    "\n",
    "* Since you want to get an unbiased estimate of your model's capacity, you decide to apply k-fold cross-validation on your corpus. To do this, implement the function `k_validation_folds` in `exercise_3.py`. Use it to split your corpus into $k=5$ cross-validation folds, and make sure that the folds are of the same size. (1 point)\n",
    "\n",
    "* Now, estimate a trigram language model on each of the CV folds. Use the `NGramLM` class, and average over all perplexity scores. Does the average score differ from the one obtained in 3.2, and why? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b10f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross-validation\n",
    "cv_folds = exercise_3.k_validation_folds(corpus, k_folds=5)\n",
    "pps = []\n",
    "\n",
    "# TODO: estimate 5 trigram LMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5ba0",
   "metadata": {},
   "source": [
    "### 3.3 Hyperparameter Tuning\n",
    "\n",
    "* As you don't know anything about the language the book is written in, you have to find the best hyperparemter $\\alpha$ for your model by a brute-force search. Since you know from 3.2 that your data is not balanced, you decide to use only the averaged perplexity score (derived from $k=5$ CV folds) for this. Do so by completing the loop in the code cell below. Then, plot the obtained perplexity scores vs. $\\alpha$. Implement the function `plot_pp_vs_alpha` in `exercise_3.py` for this. (1 point)\n",
    "\n",
    "**Hint:** This could take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamter tuning, CV for trigram\n",
    "alphas = [x*0.01 for x in range(1,101)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d91e0",
   "metadata": {},
   "source": [
    "* Repeat the tuning process for unigram and bigram language models. Does your estimate of $\\alpha$ differ? Why? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamter tuning, unigram + bigram\n",
    "\n",
    "for alpha in alphas:\n",
    "  # TODO: estimate LMs!\n",
    "\n",
    "# TODO: plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cabc2",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Bonus Question (2 points)\n",
    "\n",
    "Read the Research Paper, [Tokenization and the Noiseless Channel](https://arxiv.org/pdf/2306.16842) . The author proposes a new way to evaluate tokenization techniques in NLP using information theory. Specifically, the author introduces the concept of \"channel efficiency\" which measures the information in a model, received from the tokenizer, via Rényi entropy. The authors find that a Rényi entropy with α = 2.5 correlates strongly (0.78) with BLEU scores in machine translation tasks, outperforming traditional metrics like compressed length.\n",
    "\n",
    "\n",
    "Discuss the reasoning behind using Rényi entropy as an evaluator of tokenization efficiency, over Shannon entropy. Furthermore, discuss the implications of this finding for the design of tokenizers in NLP pipelines, particularly in tasks like machine translation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
